from fastapi import FastAPI, UploadFile, File, Form, Depends
from fastapi.responses import RedirectResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from pydantic import BaseModel
from langserve import add_routes
from chat import chain as chat_chain
from chat import llm
from dotenv import load_dotenv
from typing import Optional
from PIL import Image
import json
import pytesseract
import io
import os
from langchain_chroma import Chroma  # ìµœì‹  íŒ¨í‚¤ì§€ë¡œ ì„í¬íŠ¸
from langchain_ollama import OllamaEmbeddings
from langchain.schema import Document
import uuid 
import logging
import datetime
import xml.etree.ElementTree as ET
import pandas as pd
import requests
from apscheduler.schedulers.background import BackgroundScheduler
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains.retrieval_qa.base import RetrievalQA
from data import fetch_data_prec,fetch_data_law,fetch_data_ordin


# í™˜ê²½ ì„¤ì • íŒŒì¼ ë¡œë”©
load_dotenv()

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ê°ì²´ ì´ˆê¸°í™”
app = FastAPI()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

logger.info("ì„œë²„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")

# í˜„ì¬ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
timestamp = datetime.datetime.now().isoformat()  # ISO í˜•ì‹ìœ¼ë¡œ ë‚ ì§œ ë° ì‹œê°„ ë°˜í™˜

# Tesseract OCR ê²½ë¡œ ì„¤ì •
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# Chroma ë²¡í„° DB ì„¤ì •
CHROMA_DB_DIR = "vectorstore"
PDF_DIR = "pdfs"
embeddings = OllamaEmbeddings(model="llama3.1-instruct-8b:latest")

# ê¸°ì¡´ DB ë””ë ‰í† ë¦¬ ì‚­ì œ
#if os.path.exists(CHROMA_DB_DIR):
#    import shutil
#    shutil.rmtree(CHROMA_DB_DIR)  # ë””ë ‰í† ë¦¬ ë° ê·¸ ì•ˆì˜ ë‚´ìš© ëª¨ë‘ ì‚­ì œ
    
os.makedirs(CHROMA_DB_DIR, exist_ok=True)

####################################### PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ 

def extract_text_from_pdf(pdf_path):
    try:
        loader = PyPDFLoader(pdf_path)  # PyPDFLoaderë¡œ PDF ë¡œë“œ
        pages = loader.load()  # í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë¡œë“œëœ ë¬¸ì„œ
        text = ""
        for page in pages:
            text += page.page_content  # ê° í˜ì´ì§€ì˜ ë‚´ìš© í•©ì¹˜ê¸°
    except Exception as e:
        logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path}, ì˜¤ë¥˜: {e}")
        text = ""
    return text

# ì—‘ì…€ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ëª¨ë“  ì‹œíŠ¸ í¬í•¨)
def extract_text_from_excel(excel_path):
    text = ""
    try:
        xls = pd.ExcelFile(excel_path)
        for sheet_name in xls.sheet_names:
            df = pd.read_excel(xls, sheet_name=sheet_name, dtype=str)  # ëª¨ë“  ì‹œíŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            text += df.to_string(index=False, header=False) + "\n"
    except Exception as e:
        logger.error(f"ì—‘ì…€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {excel_path}, ì˜¤ë¥˜: {e}")
    return text

# PDF/Excel íŒŒì¼ì„ ë²¡í„° DBì— ì €ì¥ 
def save_files_to_vector_db():
    try:
        vector_db = get_vector_db()  # ë²¡í„° DB ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)  # ì²­í¬ ë¶„í•  ì„¤ì •

        for file_name in os.listdir(PDF_DIR):
            file_path = os.path.join(PDF_DIR, file_name)
            text = ""
            document_type = None  # ë¬¸ì„œ íƒ€ì… ì´ˆê¸°í™”

            # íŒŒì¼ í™•ì¥ì í™•ì¸ í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¬¸ì„œ íƒ€ì… ì„¤ì •
            if file_name.endswith(".pdf"):
                text = extract_text_from_pdf(file_path)
                document_type = "pdf"
            elif file_name.endswith(".xls") or file_name.endswith(".xlsx"):
                text = extract_text_from_excel(file_path)
                document_type = "excel"
            else:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_name}")
                continue

            # íŒŒì¼ì— íƒ€ì…ì´ ì„¤ì •ë˜ì§€ ì•Šìœ¼ë©´ skip
            if not document_type:
                logger.warning(f"íŒŒì¼ íƒ€ì…ì„ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_name}")
                continue

            # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
            text_chunks = text_splitter.split_text(text)

            # ê° ì²­í¬ë¥¼ ë²¡í„°í™” ë° ì €ì¥
            for chunk in text_chunks:
                vectors = embeddings.embed_documents([chunk])  # ì„ë² ë”© ìƒì„±
                doc_id = str(uuid.uuid4())  # ê³ ìœ  ID ìƒì„±

                metadata = {
                    "id": doc_id,
                    "type": document_type,  # 'pdf', 'excel'
                    "source": file_name,  # íŒŒì¼ëª… ì €ì¥
                    "timestamp": datetime.datetime.now().isoformat()
                }

                document = Document(page_content=chunk, metadata=metadata)

                # ë²¡í„° DBì— ì €ì¥
                vector_db.add_documents(documents=[document], embeddings=vectors, ids=[doc_id])

                logger.info(f"Document saved: {doc_id}, Source: {file_name}")
                
                # documents = vector_db.get()
                # logger.info(f"ì „ì²´ ì¡°íšŒ: {documents}")  # ë²¡í„° dbì— ì €ì¥ëœ ì „ì²´ ì •ë³´ ì¶œë ¥

        logger.info("ë²¡í„° DBì— ë¬¸ì„œ ì €ì¥ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"ë²¡í„° DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}") 
        

##################################### 30ë¶„ ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± 
scheduler = BackgroundScheduler()

def fetch_all_data():
    fetch_data_prec()
    fetch_data_law()
    fetch_data_ordin()
    
     # fetch_all_dataê°€ ì™„ë£Œëœ í›„ì— save_files_to_vector_db í˜¸ì¶œ
    save_files_to_vector_db()

# êµ­ê°€ë²•ë ¹ì •ë³´ ì—‘ì…€ ì—…ë°ì´íŠ¸
scheduler.add_job(fetch_all_data, trigger='interval', hours=24)


# CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # ë°°í¬ì‹œ ë„ë©”ì¸
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)

# ê¸°ë³¸ ê²½ë¡œ("/")ì— ëŒ€í•œ ë¦¬ë‹¤ì´ë ‰ì…˜ ì²˜ë¦¬
@app.get("/")
async def redirect_root_to_docs():
    return RedirectResponse("/prompt/playground")

@app.get("/prompt/playground")
async def playground():
    return {"message": "Welcome to the Playground!"}

########### ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ ###########
# ë²¡í„° DB ì´ˆê¸°í™” í•¨ìˆ˜ (í•œ ë²ˆë§Œ ì‹¤í–‰)
def get_vector_db():
    global vector_db
    if vector_db is None:
        vector_db = Chroma(embedding_function=embeddings, persist_directory=CHROMA_DB_DIR)
        logger.info("ë²¡í„° DB ì—°ê²° ì™„ë£Œ")
    return vector_db

# ê¸€ë¡œë²Œ ë³€ìˆ˜ë¡œ ë²¡í„° DB ê°ì²´ ì„ ì–¸ (ì‹±ê¸€í„´ íŒ¨í„´ìœ¼ë¡œ ê´€ë¦¬)
vector_db = None

# ë²¡í„° DBë¥¼ ê°€ì ¸ì˜¤ê³  retriever ë° qa_chain ì„¤ì •
vector_db = get_vector_db()
retriever = vector_db.as_retriever()

# ê²€ìƒ‰ ê¸°ë°˜ QA ì²´ì¸ (ë²¡í„° DB í™œìš©)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm, 
    chain_type="map_reduce", 
    retriever=retriever,
    return_source_documents=True
)

# ë²¡í„° DB ë©”ì‹œì§€ ì €ì¥ í•¨ìˆ˜
def save_to_vector_db(messages, document_type, conversation_id, vector_db):
    try:
        # ë²¡í„° DB ê°ì²´ ê°€ì ¸ì˜¤ê¸° (get_vector_db í˜¸ì¶œ)
        vector_db = get_vector_db()
        
        # ê° ë©”ì‹œì§€ë³„ë¡œ ë²¡í„°í™” ë° ì €ì¥
        for message in messages:
            vectors = embeddings.embed_documents([message])

            # ê³ ìœ  ID ìƒì„± (UUID ì‚¬ìš©)
            doc_id = str(uuid.uuid4())  # UUIDë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ìœ  ID ìƒì„±

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            metadata = {
                "id": doc_id,
                "type": document_type,  # 'message', 'pdf', 'web'
                "source": "user_input",
                "conversation_id": conversation_id,  # ëŒ€í™” ID ì¶”ê°€
                "timestamp": datetime.datetime.now().isoformat()
            }

            # Document ê°ì²´ ìƒì„±
            document = Document(page_content=message, metadata=metadata)

            # ë²¡í„° DBì— ë¬¸ì„œ ì¶”ê°€
            vector_db.add_documents(
                documents=[document],  # Document ê°ì²´ ì „ë‹¬
                embeddings=vectors,
                ids=[doc_id]  # ê³ ìœ  ID ì „ë‹¬
            )
            
            logger.info(f"Message saved: {doc_id}, Message: {message}") # ì‹¤í–‰ëœ ê³ ìœ idì™€ ë©”ì‹œì§€ ì¶œë ¥
            
            documents = vector_db.get()
            logger.info(f"ì „ì²´ ì¡°íšŒ: {documents}") # ë²¡í„° dbì— ì €ì¥ëœ ì „ì²´ ì •ë³´ ì¶œë ¥

        logger.info("ë²¡í„° DBì— ë©”ì‹œì§€ ì €ì¥ ì„±ê³µ")
    except Exception as e:
        logger.error(f"ë²¡í„° DB ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì…ë ¥ ë°ì´í„° ëª¨ë¸
class InputChat(BaseModel):
    messages: list[str]

# ëŒ€í™”í˜• API ì—”ë“œí¬ì¸íŠ¸
@app.post("/chat")
async def chat(input: str = Form(...), file: Optional[UploadFile] = File(None),  document_type: str = "message", vector_db: Chroma = Depends(get_vector_db)):
    # vector_dbëŠ” ì´ì œ Chroma ê°ì²´ë¡œ ìë™ ì£¼ì…ë¨
    try:
        input_data = InputChat(**json.loads(input))
        result = {}

        # ëŒ€í™” ID ìƒì„±
        conversation_id = str(uuid.uuid4())

        # ì´ë¯¸ì§€ê°€ ì—…ë¡œë“œëœ ê²½ìš° OCR ì²˜ë¦¬
        if file:
            image = Image.open(io.BytesIO(await file.read()))
            ocr_text = pytesseract.image_to_string(image, lang="kor+eng")
            input_data.messages.append(ocr_text)
            result["ocr_text"] = ocr_text
            
        # # ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        # query = input_data.messages[-1]  # ìµœì‹  ë©”ì‹œì§€ ì‚¬ìš©
        # docs = retriever.invoke(query)

        # # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ LLM ì…ë ¥ì— ì¶”ê°€
        # context = "\n\n".join([doc.page_content for doc in docs]) if docs else "ê´€ë ¨ ì •ë³´ ì—†ìŒ"
        # input_data.messages.append(f"ğŸ” ì°¸ê³  ì •ë³´:\n{context}")

        # # ì±—ë´‡ ì‘ë‹µ ìƒì„±
        # result["chatbot_response"] = chat_chain.invoke(input_data.messages)    
        
        # ì±—ë´‡ ì‘ë‹µë§Œ ìƒì„± í”„ë¡¬í”„íŒ… Test
        result["chatbot_response"] = chat_chain.invoke(input_data.messages)

        # ë²¡í„° DBì— ë©”ì‹œì§€ ì €ì¥
        save_to_vector_db(input_data.messages, document_type, conversation_id, vector_db)

        return JSONResponse(content={"message": "Chat response", "type": document_type, "result": result, "input_messages": input_data.messages})

    except Exception as e:
        return JSONResponse(content={"error": str(e)}, status_code=500)


# ëŒ€í™”í˜• ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
add_routes(
    app,
    chat_chain.with_types(input_type=InputChat),
    path="/chat",
    enable_feedback_endpoint=True,
    enable_public_trace_link_endpoint=True,
    playground_type="chat",
)

# ì„œë²„ ì‹¤í–‰ ì„¤ì •
if __name__ == "__main__":
    scheduler.start()

    # ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ ì‹¤í–‰
    # fetch_all_data()

    # FastAPI ì„œë²„ ì‹¤í–‰
    uvicorn.run(app, host="0.0.0.0", port=8000)
